{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data engineering\n\nThis assignment focuses on techniques for cleaning and transforming the data to handle challenges of missing, inaccurate, or incomplete data. Please refer to [Machine Learning productionization - Data engineering](#data-engineering) to learn more.\n\nFill `____` pieces of the below implementation in order to pass the assertions.\n\n\n<!-- \n# removing rows with missing values from dataset: pandas.DataFrame dataset.dropna(inplace=True) # removing columns with ratio of missing values greater than a threshold dataset = dataset[dataset.columns[dataset.isnull().mean() <= THRESHOLD]]\n\nImputation. If the number of missing values is not small enough to be removed and not large enough to be a substantial proportion of the variable entries, you can replace the missing values in a numerical variable with the mean/median of the non-missing entries and the missing values in a categorical variable with the mode, which is the most frequent entry of the variable.\n\n# imputing missing values in each column with the mean of the corresponding # column using scikit-learn from sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy='mean') # transform the dataset imputed_dataset = imputer.fit_transform(dataset)\n\nIdentifying the group of attributes that uniquely identify a single record is very important and needs validation from a domain expert. Removing duplicates on this group leaves you with distinct records necessary for model training. This group acts as a key to performing multiple aggregate and transformations operations on the dataset like calculating rolling mean, backfilling null values, missing value imputation (details on this in next point), etc.\nTrue duplicates, i.e., instances of the same data point, are usually removed. In this way, the increase of the sample weight on these points is eliminated, and the risk of any artificial inflation in the performance metrics is reduced -->"},{"metadata":{},"cell_type":"markdown","source":"## Exploring dataset\n\n> **Learning goal**: By the end of this subsection, you should be comfortable finding general information about the data stored in pandas DataFrames.\n\nIn order to explore this functionality, we will import the modefined version of Python scikit-learn library's iconic dataset **Iris**."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.datasets import load_iris\nimport math\n\niris_df = pd.read_csv('../../../data/modefined_sklearn_iris_dataset.csv', index_col=0)\niris_df","execution_count":1,"outputs":[{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0                  5.1               3.5                1.4            0.2000\n1                  5.1               3.5                1.4            0.2000\n2                  4.7               3.2                1.3            0.2000\n3                  4.6               3.1                1.5               NaN\n4                  5.0               3.6                1.4            0.1000\n..                 ...               ...                ...               ...\n145                6.7               3.0                5.2            2.3000\n146                6.3               2.5                5.0            1.9000\n147                6.5               3.0                5.2            2.0223\n148                6.2               3.4                5.4            2.3000\n149                5.9               3.0                5.1            1.8000\n\n[150 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.1000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3000</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>6.3</td>\n      <td>2.5</td>\n      <td>5.0</td>\n      <td>1.9000</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.0223</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>6.2</td>\n      <td>3.4</td>\n      <td>5.4</td>\n      <td>2.3000</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8000</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"To start off, print the summary of a DataFrame."},{"metadata":{"trusted":true},"cell_type":"code","source":"iris_df.shape","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"(150, 4)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"```{quizdown}\n\n## How many entries the Iris dataset has?\n\n> Please refer to the output of above cell.  \n\n- [ ] 50\n- [ ] 100\n- [x] 150\n- [ ] 200\n\n```"},{"metadata":{},"cell_type":"markdown","source":"Next, let's check the actual content of the `DataFrame`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# displying first 5 rows of our iris_df\niris_df.head(5)\n\n# in the first five rows, which one's spepal length is 5.0cm?\nassert iris_df.iloc[4, 0] == 5.0","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conversely, we can check the last few rows of the DataFrame."},{"metadata":{"trusted":true},"cell_type":"code","source":"# displying last 5 rows of our `iris_df`.\niris_df.tail(5)\n\n# in the last five rows, which one's spepal width is 2.5cm?\nassert iris_df.iloc[-4, 1] == 2.5","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Takeaway**: Even just by looking at the metadata about the information in a DataFrame or the first and last few values in one, you can get an immediate idea about the size, shape, and content of the data you are dealing with."},{"metadata":{},"cell_type":"markdown","source":"## Dealing with missing data\n"},{"metadata":{},"cell_type":"markdown","source":"Missing data can cause inaccuracies as well as weak or biased results. Sometimes these can be resolved by a \"reload\" of the data, filling in the missing values with computation and code like Python, or simply just removing the value and corresponding data. There are numerous reasons for why data may be missing and the actions that are taken to resolve these missing values can be dependent on how and why they went missing in the first place.\n\n> **Learning goal**: By the end of this subsection, you should know how to replace or remove null values from DataFrames.\n"},{"metadata":{},"cell_type":"markdown","source":"In pandas, the `isnull()` and `notnull()` methods are your primary methods for detecting null data. Both return Boolean masks over your data. We will be using numpy for NaN values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"iris_isnull_df = iris_df.isnull()\n\nprint(iris_isnull_df)\n\n# find one row with missing value\nassert iris_isnull_df.iloc[3, 3] == True\nassert math.isnan(iris_df.iloc[3, 3]) == True","execution_count":5,"outputs":[{"output_type":"stream","text":"     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0                False             False              False             False\n1                False             False              False             False\n2                False             False              False             False\n3                False             False              False              True\n4                False             False              False             False\n..                 ...               ...                ...               ...\n145              False             False              False             False\n146              False             False              False             False\n147              False             False              False             False\n148              False             False              False             False\n149              False             False              False             False\n\n[150 rows x 4 columns]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get all the rows with missing data\niris_with_missing_value_df = iris_df[iris_isnull_df.any(axis=1)] #axis=1 表示跨列执行即有一列为NAN即输出   \n\n#loc基于标签和布尔数组\n#iloc基于位置\n\nassert iris_with_missing_value_df.shape[0] == 16","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dropping null values**: Beyond identifying missing values, pandas provides a convenient means `dropna` to remove null values from Series and DataFrames. (Particularly on large data sets, it is often more advisable to simply remove missing [NA] values from your analysis than deal with them in other ways.) "},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove all the rows with missing values\niris_with_dropna_on_row_df = iris_df.dropna(how='any')\n\nassert iris_with_dropna_on_row_df.shape[0] == 134","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove all the columns with missing values\niris_with_dropna_on_column_df = iris_df.dropna(axis=1,how='any') #跨列执行，故有NAN列被删除\n\nassert iris_with_dropna_on_column_df.columns.shape[0] == 0","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove all the rows with 2 (2) missing values\niris_with_dropna_2_values_on_rows_df = iris_df.dropna(thresh=2)\n\nassert iris_with_dropna_2_values_on_rows_df.shape[0] == 144\n\n# remove all the rows with 1 (3) missing values\niris_with_dropna_1_values_on_rows_df = iris_df.dropna(thresh=1)\n\nassert iris_with_dropna_1_values_on_rows_df.shape[0] == 147","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Filling null values**: Depending on your dataset, it can sometimes make more sense to fill null values with valid ones rather than drop them. You could use `isnull` to do this in place, but that can be laborious, particularly if you have a lot of values to fill. Because this is such a common task in data science, pandas provides `fillna`, which returns a copy of the Series or DataFrame with the missing values replaced with one of your choosing. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# fll all the missing values with 0\niris_with_fillna_df = iris_df.fillna(value=0)\n\n# get all the rows with missing data\niris_with_missing_value_after_fillna_df = iris_with_fillna_df.isnull().loc[iris_with_fillna_df.isnull().any(axis=1)]\n\nassert iris_with_missing_value_after_fillna_df.shape[0] == 0\n# assert iris_with_fillna_df.iloc[, 3] == -1 没明白","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# forward-fill null values, which is to use the last valid value to fill a null:\niris_with_fillna_forward_df =iris_df.fillna(method=\"ffill\")\n\n# get all the rows with missing data\niris_with_missing_value_after_fillna_forward_df = iris_with_fillna_forward_df.isnull().loc[iris_with_fillna_forward_df.isnull().any(axis=1)]\n\nassert iris_with_missing_value_after_fillna_forward_df.shape[0] == 0\nassert float(iris_with_fillna_forward_df.iloc[3, 3]) == 0.2","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# back-fill null values, which is to use the next valid value to fill a null:\niris_with_fillna_back_df = iris_df.fillna(method=\"bfill\")\n\n# get all the rows with missing data\niris_with_missing_value_after_fillna_back_df = iris_with_fillna_back_df.isnull().loc[iris_with_fillna_forward_df.isnull().any(axis=1)]\n\nassert iris_with_missing_value_after_fillna_back_df.shape[0] == 0\nassert float(iris_with_fillna_back_df.iloc[3, 3]) == 0.1","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removing duplicate data\n\nData that has more than one occurrence can produce inaccurate results and usually should be removed. This can be a common occurrence when joining two or more datasets together. However, there are instances where duplication in joined datasets contain pieces that can provide additional information and may need to be preserved.\n\n> **Learning goal**: By the end of this subsection, you should be comfortable identifying and removing duplicate values from DataFrames.\n\nIn addition to missing data, you will often encounter duplicated data in real-world datasets. Fortunately, pandas provides an easy means of detecting and removing duplicate entries."},{"metadata":{},"cell_type":"markdown","source":"**Identifying duplicates**: You can easily spot duplicate values using the `duplicated`  method in pandas, which returns a Boolean mask indicating whether an entry in a DataFrame is a duplicate of an earlier one. Let's create another example DataFrame to see this in action."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"iris_isduplicated_df = iris_df.duplicated()\n\nprint(iris_isduplicated_df)\n\n# find one row with duplicated value\nassert iris_isduplicated_df.iloc[1] == True","execution_count":13,"outputs":[{"output_type":"stream","text":"0      False\n1       True\n2      False\n3      False\n4      False\n       ...  \n145    False\n146    False\n147    False\n148    False\n149    False\nLength: 150, dtype: bool\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Dropping duplicates**: `drop_duplicates` simply returns a copy of the data for which all of the duplicated values are False:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove all the rows with duplicated values\niris_with_drop_duplicates_on_df = iris_df.drop_duplicates()\n\nassert iris_with_drop_duplicates_on_df.shape[0] == 143","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both `duplicated` and `drop_duplicates` default to consider all columns but you can specify that they examine only a subset of columns in your DataFrame:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove all the rows with duplicated values on column 'petal width (cm)'\niris_with_drop_duplicates_on_column_df = iris_df.drop_duplicates(subset=['petal width (cm)'])\n\nassert iris_with_drop_duplicates_on_column_df.shape[0] == 27","execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handle inconsistent data\n\nDepending on the source, data can have inconsistencies in how it’s presented. This can cause problems in searching for and representing the value, where it’s seen within the dataset but is not properly represented in visualizations or query results. Common formatting problems involve resolving whitespace, dates, and data types. Resolving formatting issues is typically up to the people who are using the data. For example, standards on how dates and numbers are presented can differ by country.\n\n> **Learning goal**: By the end of this subsection, you should know how to handle the inconsistent data format in the DataFrame."},{"metadata":{},"cell_type":"markdown","source":"Let's cleaning up the **4th** column `petal width (cm)` to make sure there's no data entry inconsistencies in it. Firstly, we will use a convenient method `unique` from pandas to check the unique values of this column\n\nIn pandas, the `unique` method is a convenient way to unique values based on a hash table:"},{"metadata":{"trusted":true},"cell_type":"code","source":"column_to_format = iris_df.iloc[:,3]\ncolumn_to_format_unique = column_to_format.unique()\n\nprint(column_to_format_unique)\n\n# find one row with duplicated value\nassert column_to_format_unique.shape[0] == 27","execution_count":16,"outputs":[{"output_type":"stream","text":"[0.2       nan 0.1    0.4    0.3    0.22   0.24   0.5    0.6    1.4\n 1.5    1.3    1.6    1.     1.1    1.8    1.2    1.7    1.5012 2.5\n 1.9    2.2    2.1    2.     2.4    2.3    2.0223]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Regardless the `nan` value, you may find the numeric valus are in different precision. More specifically, `1.` or `1.5012` are not in the same precision as other numbers. We want to append tailing `0` to numbers like `1.`, and round numbers like `1.5012` to `1.5`."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# firstly, let's apply `round`` to the values to make the precision all as .1f\nformatted_column = column_to_format.round(decimals=1)\n\nprint(formatted_column.unique())\n\n#assert formatted_column.unique().shape[0] == 23","execution_count":17,"outputs":[{"output_type":"stream","text":"[0.2 nan 0.1 0.4 0.3 0.5 0.6 1.4 1.5 1.3 1.6 1.  1.1 1.8 1.2 1.7 2.5 1.9\n 2.2 2.1 2.  2.4 2.3]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# now, let's add tailing 0 if needed to make numbers like 1. to be 1.0. \n# You may need to filter the nan value while processing.\nformatted_column = formatted_column.map('{:.1f}'.format,na_action='ignore')\n\nprint(formatted_column.unique())\n\nassert formatted_column.unique().shape[0] == 23","execution_count":18,"outputs":[{"output_type":"stream","text":"['0.2' nan '0.1' '0.4' '0.3' '0.5' '0.6' '1.4' '1.5' '1.3' '1.6' '1.0'\n '1.1' '1.8' '1.2' '1.7' '2.5' '1.9' '2.2' '2.1' '2.0' '2.4' '2.3']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## At last\n\nLet's apply all the methods above to make the data to be clean."},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove all rows with missing values\nno_missing_data_df = iris_df.dropna(how='any')\n\n# remove all rows with duplicated values\nno_missing_dup_data_df = no_missing_data_df.drop_duplicates()\n\n# apply the precision .1f to all the numbers\ncleand_df = no_missing_dup_data_df.round(decimals=1)\n\nassert no_missing_data_df.shape[0] == 134\nassert no_missing_dup_data_df.shape[0] == 129\nassert cleand_df[cleand_df.columns[3]].unique().shape[0] == 22","execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also, you could refer to below two for more about how to handle outlier.\n\n- [Ways to Detect and Remove the Outliers](https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba)\n- [Outlier!!! The Silent Killer](https://www.kaggle.com/code/nareshbhat/outlier-the-silent-killer)"},{"metadata":{},"cell_type":"markdown","source":"## Acknowledgements\n\nThanks to Microsoft with creating the open source course [Data Science for Beginners](https://github.com/microsoft/Data-Science-For-Beginners). It contributes some of the content in this chapter."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"7d30ee397bdd1fbf33affe9c166bdd43c2304a81a5b0731ca6c4974a84c0bfbd"}}},"nbformat":4,"nbformat_minor":4}